{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdXJiWRpQ3LuRClkuhg2oc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianMutea/LlamaIndex-Precision-and-Simplicity-in-Information-Retrieval/blob/main/LlamaIndex_Introduction_Precision_and_Simplicity_in_Information_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "30quhpl583b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c09284-036d-4ae1-cc19-7bb1ea063d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.5/943.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.2/588.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.9.14.post3 openai==1.3.8 cohere==4.37 deeplake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = \"your_activeloop_token\""
      ],
      "metadata": {
        "id": "uOnvAUMPW7rD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "GlCZZv4FYM__"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "\n",
        "docs = loader.load_data(pages=[\"Natural Language Processing\", \"Artificial Intelligence\"])\n",
        "\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLkPBAvoYXLl",
        "outputId": "4573cf88-5cd4-4c55-aebd-29fd92e28dce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Nodes"
      ],
      "metadata": {
        "id": "DJYt4__tZGpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LlamaIndex, once data is ingested as documents, it passes through a processing structure that transforms these documents into `Node` objects. Nodes are smaller, more granular data units created from the original documents. Besides their primary content, these nodes also contain metadata and contextual information.\n",
        "\n",
        "LlamaIndex features a `NodeParser` class designed to convert the content of documents into structured nodes automatically. The `SimpleNodeParser` converts a list of document objects into nodes."
      ],
      "metadata": {
        "id": "yYNzUqhgZu08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# initialize parser\n",
        "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
        "\n",
        "# parse documents into nodes\n",
        "nodes = parser.get_nodes_from_documents(docs)\n",
        "print(len(nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS_WhNzlY8bc",
        "outputId": "c2e1d713-8fa0-42ae-ba1e-de9bc550847a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indices"
      ],
      "metadata": {
        "id": "P340ulUpZpKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the heart of LlamaIndex is the capability to index and search various data formats like documents, PDFs, and database queries. Indexing is an initial step for storing information in a database; it essentially transforms the unstructured data into embeddings that capture semantic meaning and optimize the data format so it can be easily accessed and queried.\n",
        "\n",
        "LlamaIndex has a variety of index types, each fulfills a specific role. We have highlighted some of the popular index types in the following subsections.\n",
        "\n",
        "* Summary Index - extracts a summary from each document and stores it with all the nodes in that document\n",
        "\n",
        "* Vector Store Index - generates the embeddings during index construction to identify top-k similar nodes in the response query."
      ],
      "metadata": {
        "id": "brvhlIa4aKoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The crawled Wikipedia documents can be stored in a Deep Lake vector store, and an index object can be created based on its data. We can create the dataset in Activeloop and append documents to it by employing the `DeepLakeVectorStore` class."
      ],
      "metadata": {
        "id": "emsc7j1pa1Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect to the platform, use the DeepLakeVectorStore class and provide the dataset path as an argument."
      ],
      "metadata": {
        "id": "eHHkhrIFbSAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "\n",
        "my_activeloop_id = \"brianmuteak\"\n",
        "my_activeloop_dataset_name = \"LlamaIndex_intro\"\n",
        "dataset_path = f\"hub://{my_activeloop_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "# create a vecotr store\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duokIiOnZgLb",
        "outputId": "81de1ceb-1b1f-4d7c-cd76-6c8964ba5797"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://brianmuteak/LlamaIndex_intro already exists, loading from the storage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to create a storage context using the `StorageContext` class and the Deep Lake dataset as the source. Pass this storage to a `VectorStoreIndex` class to create the index (generate embeddings) and store the results on the defined dataset."
      ],
      "metadata": {
        "id": "69RQsXxMdxEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    docs, storage_context = storage_context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3yOk16TcobW",
        "outputId": "38a6b0b3-4269-4c1d-ee18-c1c129d0012b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:05<00:00,  4.66it/s]\n",
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://brianmuteak/LlamaIndex_intro', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            " embedding  embedding  (75, 1536)  float32   None   \n",
            "    id        text      (75, 1)      str     None   \n",
            " metadata     json      (75, 1)      str     None   \n",
            "   text       text      (75, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Engines"
      ],
      "metadata": {
        "id": "dFoMokb1U6Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to leverage the generated indexes to query through the information. The Query Engine is a wrapper that combines a Retriever and a Response Synthesizer into a pipeline. The pipeline uses the query string to fetch nodes and then sends them to the LLM to generate a response. A query engine can be created by calling the `as_query_engine()` method on an already-created index."
      ],
      "metadata": {
        "id": "DTYu61MyVEex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTVectorStoreIndex\n",
        "\n",
        "# index = GPTVectorStoreIndex.from_documents(docs)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What does NLP stand for?\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "HZlKiBGge6Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8652e87-8b46-4f69-e37c-38ad5d823a93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP stands for Natural Language Processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Indexes Locally\n"
      ],
      "metadata": {
        "id": "Kq4dGsxFXaTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are scenarios where saving the data on a disk might be necessary for rapid testing. The concept of storing refers to saving the index data, which includes the nodes and their associated embeddings, to disk. This is done using the `persist()` method from the `storage_context` object related to the index."
      ],
      "metadata": {
        "id": "FdfoJxN7XdHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store index as vector embeddings on the disk\n",
        "index.storage_context.persist()\n",
        "\n",
        "# This saves the data in the 'storage' by default\n",
        "# to minimize repetitive processing"
      ],
      "metadata": {
        "id": "6yaaMhe6Vns6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the index already exists in storage, you can load it directly instead of recreating it. We simply need to determine whether the index already exists on disk and proceed accordingly:"
      ],
      "metadata": {
        "id": "lLICb4dPYBTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index Storage Checks\n",
        "\n",
        "import os.path\n",
        "from llama_index import(\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "\n",
        "from llama_index import download_loader\n",
        "\n",
        "\n",
        "# check if the index already exist\n",
        "\n",
        "if not os.path.exists(\"./storage\"):\n",
        "  # If not, load the Wikipedia data and create a new index\n",
        "  WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "  loader = WikipediaReader()\n",
        "\n",
        "  docs = loader.load_data(pages=[\"Natural Language Processing\", \"Artificial Intelligence\"])\n",
        "  index = VectorStoreIndex.from_documents(docs)\n",
        "  # store index\n",
        "  index.storage_context.persist()\n",
        "\n",
        "\n",
        "else:\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"./storage\", vector_store = vector_store)\n",
        "  index = load_index_from_storage(storage_context=storage_context)"
      ],
      "metadata": {
        "id": "DiCc1g85YD34"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What Artificial Intelligence?\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onGpQhFtd5cT",
        "outputId": "2d0137b7-6e22-4946-fdef-d6b1ab1be276"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) refers to the intelligence exhibited by machines or software, as opposed to the intelligence of humans or other living beings. It is a field of study in computer science that focuses on developing and studying intelligent machines. AI technology is widely used in various industries, government sectors, and scientific research. It encompasses a range of applications such as advanced web search engines, recommendation systems, speech recognition, self-driving cars, generative and creative tools, and superhuman play in strategy games. The goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. The ultimate long-term goal of AI is to achieve general intelligence, which refers to the ability to perform any task that a human can do. AI researchers employ various problem-solving techniques, including search algorithms, mathematical optimization, formal logic, artificial neural networks, and statistical methods, drawing upon fields such as psychology, linguistics, philosophy, neuroscience, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain vs. LlamaIndex\n",
        "LangChain and LlamaIndex are designed to improve LLMs' capabilities, each with their unique strengths.\n",
        "\n",
        "**LlamaIndex**: LlamaIndex specializes in processing, structuring, and accessing private or domain-specific data, with a focus on specific LLM interactions. It works for tasks that demand high precision and quality when dealing with specialized, domain-specific data. Its main strength lies in linking Large Language Models (LLMs) to any data source.\n",
        "\n",
        "**LangChain** is dynamic, suited for context-rich interactions, and effective for applications like chatbots and virtual assistants. These features render it highly appropriate for quick prototyping and application development.\n",
        "\n",
        "While generally used independently, it is worth noting that it can be possible to combine functions from both LangChain and LlamaIndex where they have different strengths. Both can be complementary tools. We also designed a little table below to help you understand the differences better. The attached video in the course also aims to help you decide which tool you should use for your application: LlamaIndex, LangChain, OpenAI Assistants, or doing it all from scratch (yourself).\n",
        "\n",
        "Here’s a clear comparison of each to help you quickly grasp the essentials on a few relevant topics you may consider when choosing:"
      ],
      "metadata": {
        "id": "3ZoMdt9wbljj"
      }
    }
  ]
}